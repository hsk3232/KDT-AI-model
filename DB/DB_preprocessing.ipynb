{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b518481",
   "metadata": {},
   "source": [
    "# 1. 기존 DB자료 \"Tab\" → \",\" 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e87538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ygs.csv\", sep=\"\\t\", dtype=str, encoding=\"utf-8\")\n",
    "df.to_csv(\"ygs2.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b61fb",
   "metadata": {},
   "source": [
    "# 2. 토큰화 - ① 단어사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Iterable, List, Dict, Any, Optional, Tuple, Hashable\n",
    "\n",
    "# Optional imports guarded so script works without SQL deps if not used\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "RESERVED_DEFAULT = [\"<PAD>\", \"<UNK>\"]  # 0, 1\n",
    "\n",
    "@dataclass\n",
    "class Vocab:\n",
    "    name: str\n",
    "    reserved: List[str]\n",
    "    id2token: List[Any]  # reserved first, then data tokens\n",
    "    counts: Dict[str, int]  # token string repr -> count (for reference)\n",
    "    created_at: str\n",
    "    meta: Dict[str, Any]\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.id2token)\n",
    "\n",
    "    def token_to_id(self, token: Any) -> int:\n",
    "        # exact match if token equals one of reserved strings\n",
    "        for i, r in enumerate(self.reserved):\n",
    "            if token == r:\n",
    "                return i\n",
    "        # other tokens; compare to id2token list\n",
    "        try:\n",
    "            return self.id2token.index(token)\n",
    "        except ValueError:\n",
    "            # UNK id is index of \"<UNK>\"\n",
    "            try:\n",
    "                return self.reserved.index(\"<UNK>\")\n",
    "            except ValueError:\n",
    "                # if UNK missing, fallback to 1\n",
    "                return 1\n",
    "\n",
    "    def to_json(self) -> str:\n",
    "        payload = {\n",
    "            \"name\": self.name,\n",
    "            \"reserved\": self.reserved,\n",
    "            \"id2token\": self.id2token,\n",
    "            \"counts\": self.counts,\n",
    "            \"created_at\": self.created_at,\n",
    "            \"meta\": self.meta,\n",
    "        }\n",
    "        return json.dumps(payload, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(s: str) -> \"Vocab\":\n",
    "        obj = json.loads(s)\n",
    "        return Vocab(\n",
    "            name=obj[\"name\"],\n",
    "            reserved=list(obj[\"reserved\"]),\n",
    "            id2token=list(obj[\"id2token\"]),\n",
    "            counts=dict(obj.get(\"counts\", {})),\n",
    "            created_at=obj.get(\"created_at\", \"\"),\n",
    "            meta=dict(obj.get(\"meta\", {})),\n",
    "        )\n",
    "\n",
    "def _ensure_pandas():\n",
    "    if pd is None:\n",
    "        raise RuntimeError(\"pandas is required for CSV mode. Please `pip install pandas`.\")\n",
    "\n",
    "def _iter_series_values(series) -> Iterable[Any]:\n",
    "    # Drop NaN and convert numpy types to Python scalars\n",
    "    for v in series:\n",
    "        if v is None:\n",
    "            continue\n",
    "        # Handle pandas NaN\n",
    "        try:\n",
    "            if pd is not None and pd.isna(v):\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Convert numpy scalar to python scalar\n",
    "        try:\n",
    "            import numpy as np\n",
    "            if isinstance(v, (np.generic,)):\n",
    "                v = v.item()\n",
    "        except Exception:\n",
    "            pass\n",
    "        yield v\n",
    "\n",
    "def _make_counts(values: Iterable[Any]) -> Counter:\n",
    "    c = Counter()\n",
    "    for v in values:\n",
    "        c[v] += 1\n",
    "    return c\n",
    "\n",
    "def _sorted_tokens_from_counts(cnt: Counter, min_freq: int, max_size: Optional[int]) -> List[Any]:\n",
    "    # Filter by min_freq\n",
    "    items = [(tok, n) for tok, n in cnt.items() if n >= min_freq]\n",
    "    # Deterministic order: freq desc, then token asc by str()\n",
    "    items.sort(key=lambda x: (-x[1], str(x[0])))\n",
    "    tokens = [tok for tok, _ in items]\n",
    "    if max_size is not None and max_size > 0:\n",
    "        tokens = tokens[:max_size]\n",
    "    return tokens\n",
    "\n",
    "def build_vocab_from_series(\n",
    "    name: str,\n",
    "    series,\n",
    "    min_freq: int = 1,\n",
    "    max_size: Optional[int] = None,\n",
    "    reserved: Optional[List[str]] = None,\n",
    "    meta: Optional[Dict[str, Any]] = None,\n",
    ") -> Vocab:\n",
    "    if reserved is None:\n",
    "        reserved = list(RESERVED_DEFAULT)\n",
    "    cnt = _make_counts(_iter_series_values(series))\n",
    "    tokens = _sorted_tokens_from_counts(cnt, min_freq=min_freq, max_size=max_size)\n",
    "\n",
    "    # id2token = reserved + tokens\n",
    "    id2token = list(reserved) + tokens\n",
    "    # counts saved as str keys for JSON safety\n",
    "    counts_str = {str(k): int(v) for k, v in cnt.items()}\n",
    "    return Vocab(\n",
    "        name=name,\n",
    "        reserved=list(reserved),\n",
    "        id2token=id2token,\n",
    "        counts=counts_str,\n",
    "        created_at=datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "        meta=meta or {},\n",
    "    )\n",
    "\n",
    "def build_joint_vocab_from_df(\n",
    "    name: str,\n",
    "    df, cols: List[str],\n",
    "    sep: str = \"||\",\n",
    "    min_freq: int = 1,\n",
    "    max_size: Optional[int] = None,\n",
    "    reserved: Optional[List[str]] = None,\n",
    "    meta: Optional[Dict[str, Any]] = None,\n",
    ") -> Vocab:\n",
    "    # Create tuple tokens then stringify with sep for JSON-safe storage\n",
    "    tuples = df[cols].astype(str).agg(sep.join, axis=1)\n",
    "    return build_vocab_from_series(\n",
    "        name=name, series=tuples, min_freq=min_freq, max_size=max_size, reserved=reserved,\n",
    "        meta={\"joint_cols\": cols, \"sep\": sep, **(meta or {})},\n",
    "    )\n",
    "\n",
    "def encode_series_to_ids(series, vocab: Vocab) -> List[int]:\n",
    "    # Builds a list of ids for a 1D series\n",
    "    token2id = {tok: i for i, tok in enumerate(vocab.id2token)}\n",
    "    unk_id = vocab.reserved.index(\"<UNK>\") if \"<UNK>\" in vocab.reserved else 1\n",
    "    ids = []\n",
    "    for v in _iter_series_values(series):\n",
    "        ids.append(token2id.get(v, unk_id))\n",
    "    return ids\n",
    "\n",
    "def load_csv_select_column(csv_path: str, col: str, where_col: Optional[str]=None, where_val: Optional[str]=None):\n",
    "    _ensure_pandas()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if where_col is not None and where_val is not None:\n",
    "        df = df[df[where_col] == where_val]\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in CSV.\")\n",
    "    return df[col]\n",
    "\n",
    "def load_csv_dataframe(csv_path: str, usecols: Optional[List[str]]=None, where_col: Optional[str]=None, where_val: Optional[str]=None):\n",
    "    _ensure_pandas()\n",
    "    df = pd.read_csv(csv_path, usecols=usecols)\n",
    "    if where_col is not None and where_val is not None:\n",
    "        df = df[df[where_col] == where_val]\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Build vocabulary JSON from CSV or SQL query.\")\n",
    "    src = ap.add_mutually_exclusive_group(required=True)\n",
    "    src.add_argument(\"--csv\", type=str, help=\"CSV file path\")\n",
    "    src.add_argument(\"--sql\", type=str, help=\"SQLAlchemy URL, e.g., mysql+pymysql://user:pwd@host:3306/db\")\n",
    "\n",
    "    ap.add_argument(\"--query\", type=str, help=\"SQL query when using --sql\")\n",
    "    ap.add_argument(\"--col\", type=str, help=\"Single column name to build vocab from\")\n",
    "    ap.add_argument(\"--joint-cols\", nargs=\"+\", help=\"Build joint vocab from multiple columns\")\n",
    "    ap.add_argument(\"--where-col\", type=str, help=\"Column name for filtering (e.g., split)\")\n",
    "    ap.add_argument(\"--where-val\", type=str, help=\"Value for filtering (e.g., train)\")\n",
    "    ap.add_argument(\"--min-freq\", type=int, default=1, help=\"Minimum frequency to keep a token\")\n",
    "    ap.add_argument(\"--max-size\", type=int, default=0, help=\"Max vocab size (0 = unlimited)\")\n",
    "    ap.add_argument(\"--out\", type=str, required=True, help=\"Output JSON path\")\n",
    "    ap.add_argument(\"--name\", type=str, default=\"\", help=\"Optional name override for vocab\")\n",
    "    ap.add_argument(\"--reserved\", nargs=\"*\", default=None, help=\"Reserved tokens in order (default: <PAD> <UNK>)\")\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if args.csv:\n",
    "        if args.joint_cols:\n",
    "            df = load_csv_dataframe(args.csv, usecols=args.joint_cols + ([args.where_col] if args.where_col else []), where_col=args.where_col, where_val=args.where_val)\n",
    "            vocab = build_joint_vocab_from_df(\n",
    "                name=args.name or (\"joint:\" + \"+\".join(args.joint_cols)),\n",
    "                df=df,\n",
    "                cols=args.joint_cols,\n",
    "                min_freq=args.min_freq,\n",
    "                max_size=(args.max_size or None),\n",
    "                reserved=args.reserved,\n",
    "                meta={\"source\": \"csv\", \"csv\": args.csv, \"filter\": {args.where_col: args.where_val} if args.where_col else None},\n",
    "            )\n",
    "        else:\n",
    "            if not args.col:\n",
    "                print(\"Error: --col is required when not using --joint-cols\", file=sys.stderr)\n",
    "                sys.exit(2)\n",
    "            series = load_csv_select_column(args.csv, args.col, where_col=args.where_col, where_val=args.where_val)\n",
    "            vocab = build_vocab_from_series(\n",
    "                name=args.name or args.col,\n",
    "                series=series,\n",
    "                min_freq=args.min_freq,\n",
    "                max_size=(args.max_size or None),\n",
    "                reserved=args.reserved,\n",
    "                meta={\"source\": \"csv\", \"csv\": args.csv, \"filter\": {args.where_col: args.where_val} if args.where_col else None},\n",
    "            )\n",
    "    else:\n",
    "        # SQL mode\n",
    "        if not args.query:\n",
    "            print(\"Error: --query is required with --sql\", file=sys.stderr)\n",
    "            sys.exit(2)\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            from sqlalchemy import create_engine\n",
    "        except Exception as e:\n",
    "            print(\"Error: SQL mode requires pandas and sqlalchemy. pip install pandas sqlalchemy [driver]\", file=sys.stderr)\n",
    "            sys.exit(2)\n",
    "        engine = create_engine(args.sql)\n",
    "        df = pd.read_sql(args.query, engine)\n",
    "        if args.joint_cols:\n",
    "            for c in args.joint_cols:\n",
    "                if c not in df.columns:\n",
    "                    raise ValueError(f\"Column '{c}' not found in SQL result.\")\n",
    "            vocab = build_joint_vocab_from_df(\n",
    "                name=args.name or (\"joint:\" + \"+\".join(args.joint_cols)),\n",
    "                df=df,\n",
    "                cols=args.joint_cols,\n",
    "                min_freq=args.min_freq,\n",
    "                max_size=(args.max_size or None),\n",
    "                reserved=args.reserved,\n",
    "                meta={\"source\": \"sql\", \"query\": args.query},\n",
    "            )\n",
    "        else:\n",
    "            if not args.col:\n",
    "                print(\"Error: --col is required when not using --joint-cols\", file=sys.stderr)\n",
    "                sys.exit(2)\n",
    "            if args.col not in df.columns:\n",
    "                raise ValueError(f\"Column '{args.col}' not found in SQL result.\")\n",
    "            series = df[args.col]\n",
    "            vocab = build_vocab_from_series(\n",
    "                name=args.name or args.col,\n",
    "                series=series,\n",
    "                min_freq=args.min_freq,\n",
    "                max_size=(args.max_size or None),\n",
    "                reserved=args.reserved,\n",
    "                meta={\"source\": \"sql\", \"query\": args.query},\n",
    "            )\n",
    "\n",
    "    with open(args.out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(vocab.to_json())\n",
    "\n",
    "    print(f\"Wrote vocab: {args.out} (size={vocab.size})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
